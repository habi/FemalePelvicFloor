{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try do do a 3D registration of the pelvic floor scans\n",
    "This notebook is *heavily* based on http://insightsoftwareconsortium.github.io/SimpleITK-Notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "import pandas\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "import dask_image.imread\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure defaults\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')  # Display all images in b&w and with 'nearest' interpolation\n",
    "plt.rcParams['figure.figsize'] = (16, 9)  # Size up figures a bit\n",
    "plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scale bar defaults\n",
    "plt.rcParams['scalebar.location'] = 'lower right'\n",
    "plt.rcParams['scalebar.frameon'] = False\n",
    "plt.rcParams['scalebar.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different locations if running either on Linux or Windows\n",
    "FastSSD = True\n",
    "# to speed things up significantly\n",
    "if 'Linux' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "    else:\n",
    "        BasePath = os.path.join(os.sep, 'home', 'habi', 'research-storage-djonov')\n",
    "elif 'Darwin' in platform.system():\n",
    "    # First mount smb://resstore.unibe.ch/ana_rs_djonov/data in the Finder\n",
    "    FastSSD = False\n",
    "    BasePath = os.path.join('/Volumes/data/')\n",
    "elif 'Windows' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        if 'anaklin' in platform.node():\n",
    "            BasePath = os.path.join('V:\\\\')\n",
    "        else:\n",
    "            BasePath = os.path.join('V:\\\\')\n",
    "Root = os.path.join(BasePath, 'Aaldijk')\n",
    "print('We are loading all the data from %s' % Root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make us a dataframe for saving all that we need\n",
    "Data = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get *all* log files\n",
    "Data['LogFile'] = [f for f in sorted(glob.glob(os.path.join(Root, '**', '*.log'), recursive=True))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get *all* log files\n",
    "Data['LogFile'] = [f for f in sorted(glob.glob(os.path.join(Root, '**', '*.log'), recursive=True))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all folders\n",
    "Data['Folder'] = [os.path.dirname(f) for f in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for samples which are not yet reconstructed\n",
    "for c, row in Data.iterrows():\n",
    "    # Iterate over every 'proj' folder\n",
    "    if 'proj' in row.Folder:\n",
    "        if not 'TScopy' in row.Folder and not 'PR' in row.Folder:\n",
    "            # If there's nothing with 'rec*' on the same level, then tell us        \n",
    "            if not glob.glob(row.Folder.replace('proj', '*rec*')):\n",
    "                print('- %s is missing matching reconstructions' % row.LogFile[len(Root)+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['XYAlignment'] = [glob.glob(os.path.join(f, '*.csv')) for f in Data['Folder']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for samples which are missing the .csv-files for the XY-alignment\n",
    "for c, row in Data.iterrows():\n",
    "    # Iterate over every 'proj' folder\n",
    "    if 'proj' in row.Folder:\n",
    "        if not len(row.XYAlignment):\n",
    "            if not any(x in row.LogFile for x in ['rectmp.log']):\n",
    "                # 'rectmp.log' because we only exclude it afterwards :)\n",
    "                print('- %s has *not* been X/Y aligned' % row.LogFile[len(Root)+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all logfiles we don't want\n",
    "for c, row in Data.iterrows():\n",
    "    if 'rec' not in row.Folder:  # drop all non-rec folders\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'SubScan' in row.Folder:  # drop all partial reconstructions which might be there from synchronization\n",
    "        Data.drop([c], inplace=True)        \n",
    "    elif 'rectmp.log' in row.LogFile:  # drop all temporary logfiles\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'Foetus01' in row.LogFile:  # drop this sample, which we don't need\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'rec_old' in row.LogFile:  # drop the 'old' reconstructions with varying gray values\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate us some meaningful colums\n",
    "Data['Sample'] = [l[len(Root)+1:].split(os.sep)[0] for l in Data['LogFile']]\n",
    "Data['Scan'] = ['_'.join(l[len(Root)+1:].split(os.sep)[1:-1]) for l in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file names of the reconstructions\n",
    "Data['Reconstructions'] = [sorted(glob.glob(os.path.join(f, '*rec0*.png'))) for f in Data['Folder']]\n",
    "Data['Number of reconstructions'] = [len(r) for r in Data.Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixelsize(logfile):\n",
    "    \"\"\"Get the pixel size from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Image Pixel' in line and 'Scaled' not in line:\n",
    "                pixelsize = float(line.split('=')[1])\n",
    "    return(pixelsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Voxelsize'] = [get_pixelsize(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scandate(logfile, verbose=False):\n",
    "    \"\"\"When did we scan the Sample?\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Study Date and Time' in line:\n",
    "                if verbose:\n",
    "                    print('Found \"date\" line: %s' % line.strip())\n",
    "                datestring = line.split('=')[1].strip().replace('  ', ' ')\n",
    "                if verbose:\n",
    "                    print('The date string is: %s' % datestring)\n",
    "                date = pandas.to_datetime(datestring , format='%d %b %Y %Hh:%Mm:%Ss')\n",
    "                if verbose:\n",
    "                    print('Parsed to: %s' % date)\n",
    "    return(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Scan date'] = [get_scandate(log) for log in Data['LogFile']]\n",
    "Data['Time passed'] = [sd - Data['Scan date'].min() for sd in Data['Scan date']]\n",
    "Data['Days passed'] = [t.round('d') for t in Data['Time passed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop high-resolution scans, e.g. only register the quick ones...\n",
    "Data = Data[Data.Voxelsize > 15]\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all reconstructions into ephemereal DASK arrays\n",
    "Reconstructions = [None] * len(Data)\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   desc='Load reconstructions',\n",
    "                   total=len(Data)):\n",
    "    Reconstructions[c] = dask_image.imread.imread(os.path.join(row['Folder'],\n",
    "                                                               '*rec*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View it in 3D\n",
    "# viewer = napari.view_image(Reconstructions[14][::4,::4,::4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How big are the datasets?\n",
    "Data['Size'] = [rec.shape for rec in Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The three cardinal directions\n",
    "directions = ['Axial',\n",
    "              'Coronal',\n",
    "              'Sagittal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the middle slices, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Mid_' + direction] = [None] * len(Reconstructions)\n",
    "for c, row in tqdm(Data.iterrows(), desc='Middle images', total=len(Data), leave=False):\n",
    "    for d, direction in tqdm(enumerate(directions),\n",
    "                             desc='%s/%s' % (row['Sample'], row['Scan']),\n",
    "                             leave=False,\n",
    "                             total=len(directions)):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                                   '%s.%s.Middle.%s.png' % (row['Sample'],\n",
    "                                                            row['Scan'],\n",
    "                                                            direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c, 'Mid_' + direction] = dask_image.imread.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate requested axial view\n",
    "            if 'Axial' in direction:\n",
    "                Data.at[c, 'Mid_' + direction] = Reconstructions[c][Data['Size'][c][0] // 2].squeeze()[:,:,1]\n",
    "            if 'Sagittal' in direction:\n",
    "                Data.at[c, 'Mid_' + direction] = Reconstructions[c][:, Data['Size'][c][1] // 2, :].squeeze()[:,:,1]\n",
    "            if 'Coronal' in direction:\n",
    "                Data.at[c, 'Mid_' + direction] = Reconstructions[c][:, :, Data['Size'][c][2] // 2].squeeze()[:,:,1]\n",
    "            # Save the calculated 'direction' view to disk           \n",
    "            imageio.imwrite(outfilepath, Data.at[c, 'Mid_' + direction].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the directional MIPs, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['MIP_' + direction] = [None] * len(Reconstructions)\n",
    "for c, row in tqdm(Data.iterrows(), desc='MIPs', total=len(Data), leave=False):\n",
    "    for d, direction in tqdm(enumerate(directions),\n",
    "                             desc='%s/%s' % (row['Sample'], row['Scan']),\n",
    "                             leave=False,\n",
    "                             total=len(directions)):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                                   '%s.%s.MIP.%s.png' % (row['Sample'],\n",
    "                                                      row['Scan'],\n",
    "                                                      direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c, 'MIP_' + direction] = dask_image.imread.imread(outfilepath).squeeze()\n",
    "        else:\n",
    "            # Generate MIP\n",
    "            Data.at[c, 'MIP_' + direction] = Reconstructions[c].max(axis=d).compute()\n",
    "            # Save it out\n",
    "            imageio.imwrite(outfilepath, Data.at[c, 'MIP_' + direction].astype('uint8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Actually start the registration now\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "from downloaddata import fetch_data as fdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, fixed\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Callback invoked by the interact IPython method for scrolling through the image stacks of\n",
    "# the two images (moving and fixed).\n",
    "def display_images(fixed_image_z, moving_image_z, fixed_npa, moving_npa):\n",
    "    # Create a figure with two subplots and the specified size.\n",
    "    plt.subplots(1, 2, figsize=(10, 8))\n",
    "\n",
    "    # Draw the fixed image in the first subplot.\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(fixed_npa[fixed_image_z, :, :], cmap=plt.cm.Greys_r)\n",
    "    plt.title(\"fixed image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Draw the moving image in the second subplot.\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(moving_npa[moving_image_z, :, :], cmap=plt.cm.Greys_r)\n",
    "    plt.title(\"moving image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Callback invoked by the IPython interact method for scrolling and modifying the alpha blending\n",
    "# of an image stack of two images that occupy the same physical space.\n",
    "def display_images_with_alpha(image_z, alpha, fixed, moving):\n",
    "    img = (1.0 - alpha) * fixed[:, :, image_z] + alpha * moving[:, :, image_z]\n",
    "    plt.imshow(sitk.GetArrayViewFromImage(img), cmap=plt.cm.Greys_r)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Callback invoked when the StartEvent happens, sets up our new data.\n",
    "def start_plot():\n",
    "    global metric_values, multires_iterations\n",
    "\n",
    "    metric_values = []\n",
    "    multires_iterations = []\n",
    "\n",
    "# Callback invoked when the EndEvent happens, do cleanup of data and figure.\n",
    "def end_plot():\n",
    "    global metric_values, multires_iterations\n",
    "\n",
    "    del metric_values\n",
    "    del multires_iterations\n",
    "    # Close figure, we don't want to get a duplicate of the plot latter on.\n",
    "    plt.close()\n",
    "\n",
    "# Callback invoked when the IterationEvent happens, update our data and display new figure.\n",
    "def plot_values(registration_method):\n",
    "    global metric_values, multires_iterations\n",
    "\n",
    "    metric_values.append(registration_method.GetMetricValue())\n",
    "    # Clear the output area (wait=True, to reduce flickering), and plot current data\n",
    "    clear_output(wait=True)\n",
    "    # Plot the similarity metric values\n",
    "    plt.plot(metric_values, \"r\")\n",
    "    plt.plot(\n",
    "        multires_iterations,\n",
    "        [metric_values[index] for index in multires_iterations],\n",
    "        \"b*\",\n",
    "    )\n",
    "    plt.xlabel(\"Iteration Number\", fontsize=12)\n",
    "    plt.ylabel(\"Metric Value\", fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "# Callback invoked when the sitkMultiResolutionIterationEvent happens, update the index into the\n",
    "# metric_values list.\n",
    "def update_multires_iterations():\n",
    "    global metric_values, multires_iterations\n",
    "    multires_iterations.append(len(metric_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Sample', 'Folder', 'Scan', 'Voxelsize']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to downsample the image to be actually able to register them\n",
    "downsample = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use last scan as fixed image, try to register all other scans to it\n",
    "fixed_image = sitk.GetImageFromArray(Reconstructions[len(Data)-1][::downsample,::downsample,::downsample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load another scan to register\n",
    "whichone = 0\n",
    "moving_image = sitk.GetImageFromArray(Reconstructions[whichone][::downsample,::downsample,::downsample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize transformation (as is from SimpleITK notebook)\n",
    "initial_transform = sitk.CenteredTransformInitializer(fixed_image,\n",
    "                                                      moving_image,\n",
    "                                                      sitk.Euler3DTransform(),\n",
    "                                                      sitk.CenteredTransformInitializerFilter.GEOMETRY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied as is from SimpleITK notebook\n",
    "registration_method = sitk.ImageRegistrationMethod()\n",
    "\n",
    "# Similarity metric settings.\n",
    "registration_method.SetMetricAsMattesMutualInformation(numberOfHistogramBins=50)\n",
    "registration_method.SetMetricSamplingStrategy(registration_method.RANDOM)\n",
    "registration_method.SetMetricSamplingPercentage(0.01)\n",
    "\n",
    "registration_method.SetInterpolator(sitk.sitkLinear)\n",
    "\n",
    "# Optimizer settings.\n",
    "registration_method.SetOptimizerAsGradientDescent(\n",
    "    learningRate=1.0,\n",
    "    numberOfIterations=100,\n",
    "    convergenceMinimumValue=1e-6,\n",
    "    convergenceWindowSize=10,\n",
    ")\n",
    "registration_method.SetOptimizerScalesFromPhysicalShift()\n",
    "\n",
    "# Setup for the multi-resolution framework.\n",
    "registration_method.SetShrinkFactorsPerLevel(shrinkFactors=[4, 2, 1])\n",
    "registration_method.SetSmoothingSigmasPerLevel(smoothingSigmas=[2, 1, 0])\n",
    "registration_method.SmoothingSigmasAreSpecifiedInPhysicalUnitsOn()\n",
    "\n",
    "# Don't optimize in-place, we would possibly like to run this cell multiple times.\n",
    "registration_method.SetInitialTransform(initial_transform, inPlace=False)\n",
    "\n",
    "# Connect all of the observers so that we can perform plotting during registration.\n",
    "registration_method.AddCommand(sitk.sitkStartEvent, start_plot)\n",
    "registration_method.AddCommand(sitk.sitkEndEvent, end_plot)\n",
    "registration_method.AddCommand(sitk.sitkMultiResolutionIterationEvent, update_multires_iterations)\n",
    "registration_method.AddCommand(sitk.sitkIterationEvent, lambda: plot_values(registration_method))\n",
    "\n",
    "final_transform = registration_method.Execute(sitk.Cast(fixed_image,\n",
    "                                                        sitk.sitkFloat32),\n",
    "                                              sitk.Cast(moving_image,\n",
    "                                                        sitk.sitkFloat32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final metric value: {registration_method.GetMetricValue()}\")\n",
    "print(f\"Optimizer's stopping condition, {registration_method.GetOptimizerStopConditionDescription()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should probably save the transformation somewhere\n",
    "# print(final_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample image\n",
    "moving_resampled = sitk.Resample(moving_image,\n",
    "                                 fixed_image,\n",
    "                                 final_transform,\n",
    "                                 sitk.sitkLinear,\n",
    "                                 0.0,\n",
    "                                 moving_image.GetPixelID())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out registered image\n",
    "moving_resampled_filename = os.path.join(os.path.dirname(Data.Folder[whichone]),\n",
    "                                         '%s.registered_with.%s.downsampled_%sx.tif' % (Data.Sample[whichone],\n",
    "                                                                                        Data.Sample[whichone],\n",
    "                                                                                        downsample))\n",
    "if not os.path.exists(moving_resampled_filename):\n",
    "    print('Saving registered image to %s' % moving_resampled_filename[len(Root):])\n",
    "    sitk.WriteImage(moving_resampled, moving_resampled_filename)\n",
    "else:\n",
    "    print('Already written to %s' % moving_resampled_filename[len(Root):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fixed image, resampled\n",
    "fixed_resampled_filename = os.path.join(os.path.dirname(Data.Folder[len(Data)-1]),\n",
    "                                        '%s.downsampled.%sx.tif' % (Data.Sample[len(Data)-1],\n",
    "                                                                    downsample))\n",
    "if not os.path.exists(fixed_resampled_filename):\n",
    "    print('Saving resampled fixed image to %s' % fixed_resampled_filename[len(Root):])        \n",
    "    sitk.WriteImage(fixed_image, fixed_resampled_filename)\n",
    "else:\n",
    "    print('Already written to %s' % fixed_resampled_filename[len(Root):])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images in again\n",
    "img_fixed = dask_image.imread.imread(fixed_resampled_filename)\n",
    "img_registered = dask_image.imread.imread(moving_resampled_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show what we did there\n",
    "plt.subplot(131)\n",
    "plt.imshow(img_fixed[img_fixed.shape[0]//2])\n",
    "plt.title(Data.Sample[len(Data)-1])\n",
    "plt.gca().add_artist(ScaleBar(Data['Voxelsize'][len(Data)-1]*downsample,'um'))\n",
    "plt.axis('off')\n",
    "plt.subplot(132)\n",
    "plt.imshow(img_registered[img_registered.shape[0]//2])\n",
    "plt.title('%s, registered' % Data.Sample[whichone])\n",
    "plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichone]*downsample,'um'))\n",
    "plt.axis('off')\n",
    "plt.subplot(133)\n",
    "plt.imshow(Reconstructions[whichone][::downsample,::downsample,::downsample][Reconstructions[whichone][::downsample,::downsample,::downsample].shape[0]//2])\n",
    "plt.title('%s, original' % Data.Sample[whichone])\n",
    "plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichone]*downsample,'um'))\n",
    "plt.axis('off')\n",
    "plt.savefig(os.path.join(os.path.dirname(Data.Folder[len(Data)-1]),\n",
    "                         'Registration.%s.with.%s.png' % (Data['Sample'][whichone], Data['Sample'][len(Data)-1])),\n",
    "            transparent=True,\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
