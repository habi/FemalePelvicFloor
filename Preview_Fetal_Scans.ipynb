{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datawrangling the pelvic floor scans of the fetal sample\n",
    "Let's see what we did there..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "import glob\n",
    "import pandas\n",
    "import dask\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask_image.imread\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn\n",
    "import numpy\n",
    "from tqdm.auto import tqdm, trange\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import our own parsing functions which we've added as submodule\n",
    "from BrukerSkyScanLogfileRuminator.parsing_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set dask temporary folder\n",
    "# Do this before creating a client: https://stackoverflow.com/a/62804525/323100\n",
    "import tempfile\n",
    "if 'Linux' in platform.system():\n",
    "    tmp = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "elif 'Darwin' in platform.system():\n",
    "    tmp = tempfile.gettempdir()\n",
    "else:\n",
    "    if 'anaklin' in platform.node():\n",
    "        tmp = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        tmp = os.path.join('D:\\\\')\n",
    "dask.config.set({'temporary_directory': os.path.join(tmp, 'tmp')})\n",
    "print('Dask temporary files go to %s' % dask.config.get('temporary_directory'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start cluster and client now, after setting tempdir\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('You can seee what DASK is doing at \"http://localhost:%s/status\"' % client.scheduler_info()['services']['dashboard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Ignore warnings in the notebook\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up figure defaults\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')  # Display all images in b&w and with 'nearest' interpolation\n",
    "plt.rcParams['figure.figsize'] = (16, 9)  # Size up figures a bit\n",
    "plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup scale bar defaults\n",
    "plt.rcParams['scalebar.location'] = 'lower right'\n",
    "plt.rcParams['scalebar.frameon'] = False\n",
    "plt.rcParams['scalebar.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display all plots identically\n",
    "lines = 3\n",
    "# And then do something like\n",
    "# plt.subplot(lines, int(numpy.ceil(len(Data) / float(lines))), c + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Different locations if running either on Linux or Windows\n",
    "FastSSD = False\n",
    "# to speed things up significantly\n",
    "if 'Linux' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "    else:\n",
    "        BasePath = os.path.join(os.sep, 'home', 'habi', 'research-storage-djonov')\n",
    "elif 'Darwin' in platform.system():\n",
    "    # First mount smb://resstore.unibe.ch/ana_rs_djonov/data in the Finder\n",
    "    FastSSD = False\n",
    "    BasePath = os.path.join('/Volumes/data/')\n",
    "elif 'Windows' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        if 'anaklin' in platform.node():\n",
    "            BasePath = os.path.join('V:\\\\')\n",
    "        else:\n",
    "            BasePath = os.path.join('V:\\\\')\n",
    "Root = os.path.join(BasePath, 'Aaldijk', 'PelvicFloor')\n",
    "print('We are loading all the data from %s' % Root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_git_hash():\n",
    "    '''\n",
    "    Get the current git hash from the repository.\n",
    "    Based on http://stackoverflow.com/a/949391/323100 and\n",
    "    http://stackoverflow.com/a/18283905/323100\n",
    "    '''\n",
    "    from subprocess import Popen, PIPE\n",
    "    import os\n",
    "    gitprocess = Popen(['git',\n",
    "                        '--git-dir',\n",
    "                        os.path.join(os.getcwd(), '.git'),\n",
    "                        'rev-parse',\n",
    "                        '--short',\n",
    "                        '--verify',\n",
    "                        'HEAD'],\n",
    "                       stdout=PIPE)\n",
    "    (output, _) = gitprocess.communicate()\n",
    "    return output.strip().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make directory for output\n",
    "OutPutDir = os.path.join(os.getcwd(), 'Output', get_git_hash())\n",
    "print('We are saving all the output to %s' % OutPutDir)\n",
    "os.makedirs(OutPutDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make us a dataframe for saving all that we need\n",
    "Data = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get *all* log files\n",
    "Data['LogFile'] = [f for f in sorted(glob.glob(os.path.join(Root, '**', '*.log'), recursive=True))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop all non-foetus scans\n",
    "for c, row in Data.iterrows():\n",
    "    if 'Foetus02' not in row.LogFile:\n",
    "        Data.drop([c], inplace=True)\n",
    "Data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all folders\n",
    "Data['Folder'] = [os.path.dirname(f) for f in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for samples which are not yet reconstructed\n",
    "for c, row in Data.iterrows():\n",
    "    # Iterate over every 'proj' folder\n",
    "    if 'proj' in row.Folder:\n",
    "        if not 'TScopy' in row.Folder and not 'PR' in row.Folder:\n",
    "            # If there's nothing with 'rec*' on the same level, then tell us        \n",
    "            if not glob.glob(row.Folder.replace('proj', '*rec*')):\n",
    "                print('- %s is missing matching reconstructions' % row.LogFile[len(Root)+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data['XYAlignment'] = [glob.glob(os.path.join(f, '*.csv')) for f in Data['Folder']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for samples which are missing the .csv-files for the XY-alignment\n",
    "for c, row in Data.iterrows():\n",
    "    # Iterate over every 'proj' folder\n",
    "    if 'proj' in row.Folder:\n",
    "        if not len(row.XYAlignment):\n",
    "            if not any(x in row.LogFile for x in ['rectmp.log']):\n",
    "                # 'rectmp.log' because we only exclude it afterwards :)\n",
    "                print('- %s has *not* been X/Y aligned' % row.LogFile[len(Root)+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get rid of all logfiles that we don't want\n",
    "for c, row in Data.iterrows():\n",
    "    if 'rec' not in row.Folder:  # drop all non-rec folders\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'SubScan' in row.Folder:  # drop all partial reconstructions which might be there from synchronization\n",
    "        Data.drop([c], inplace=True)        \n",
    "    elif 'rectmp.log' in row.LogFile:  # drop all temporary logfiles\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate us some meaningful colums\n",
    "Data['Sample'] = [l[len(Root)+1:].split(os.sep)[0] for l in Data['LogFile']]\n",
    "Data['SampleName'] = [sn.split('_')[0] for sn in Data['Sample']]\n",
    "Data['Scan'] = ['_'.join(l[len(Root)+1:].split(os.sep)[1:-1]) for l in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if not len(Data.SampleName.unique()) == 1:\n",
    "#     print('Something went wrong with the extration of the common sample name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.SampleName.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the file names of the reconstructions\n",
    "Data['Reconstructions'] = [sorted(glob.glob(os.path.join(f, '*rec0*.png'))) for f in Data['Folder']]\n",
    "Data['Number of reconstructions'] = [len(r) for r in Data.Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop samples which have either not been reconstructed yet or of which we deleted the reconstructions with\n",
    "# `find . -name \"*rec*.png\" -type f -mtime +333 -delete`\n",
    "# Based on https://stackoverflow.com/a/13851602\n",
    "# for c,row in Data.iterrows():\n",
    "#     if not row['Number of reconstructions']:\n",
    "#         print('%s contains no PNG files, we might be currently reconstructing it' % row.Folder)\n",
    "Data = Data[Data['Number of reconstructions'] > 0]\n",
    "Data.reset_index(drop=True, inplace=True)\n",
    "print('We have %s folders with reconstructions' % (len(Data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get scanning parameters to doublecheck from logfiles\n",
    "Data['Scanner'] = [scanner(log) for log in Data['LogFile']]\n",
    "Data['Voltage'] = [voltage(log) for log in Data['LogFile']]\n",
    "Data['Current'] = [current(log) for log in Data['LogFile']]\n",
    "Data['Voxelsize'] = [pixelsize(log, rounded=True) for log in Data['LogFile']]\n",
    "Data['CameraWindow'] = [projection_size(log) for log in Data['LogFile']]\n",
    "Data['Exposuretime'] = [exposure(log) for log in Data['LogFile']]\n",
    "Data['Averaging'] = [averaging(log) for log in Data['LogFile']]\n",
    "Data['Stacks'] = [stacks(log) for log in Data['LogFile']]\n",
    "Data['RotationStep'] = [rotationstep(log) for log in Data['LogFile']]\n",
    "Data['Scan date'] = [scandate(log) for log in Data['LogFile']]\n",
    "Data['Scan time'] = [duration(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data.Voltage.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data.Current.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data.Averaging.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data.Voxelsize.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get reconstruction parameters to doublecheck from logfiles\n",
    "Data['Grayvalue'] = [reconstruction_grayvalue(log) for log in Data['LogFile']]\n",
    "Data['RingartefactCorrection'] = [ringremoval(log) for log in Data['LogFile']]\n",
    "Data['BeamHardeningCorrection'] = [beamhardening(log) for log in Data['LogFile']]\n",
    "Data['DefectPixelMasking'] = [defectpixelmasking(log) for log in Data['LogFile']]\n",
    "Data['ROI'] = [region_of_interest(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data['Grayvalue'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data['RingartefactCorrection'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sort our dataframe by scan date\n",
    "Data.sort_values(by='Scan date', inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c,row in Data.iterrows():\n",
    "    if row.Grayvalue != 0.04:\n",
    "        print(row.Sample, row.Scan, row.Grayvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c,row in Data.iterrows():\n",
    "    if not row.RingartefactCorrection:\n",
    "        print(row.Sample, row.Scan, row.RingartefactCorrection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data[['Sample', 'Scan', 'Grayvalue', 'RingartefactCorrection', 'BeamHardeningCorrection', 'DefectPixelMasking']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate time 'spent' since start\n",
    "Data['Time passed'] = [sd - Data['Scan date'].min() for sd in Data['Scan date']]\n",
    "# Also extract days, rounded\n",
    "Data['Days passed'] = [t.round('d') for t in Data['Time passed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data[['Sample', 'Scan date', 'Time passed', 'Days passed']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some consistency checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check ringremoval parameters\n",
    "for machine in Data['Scanner'].unique():\n",
    "    print('For the %s we have '\n",
    "          'ringartefact-correction values of %s' % (machine,\n",
    "                                                    Data[Data.Scanner==machine]['RingartefactCorrection'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check beamhardening parameters\n",
    "for scanner in Data.Scanner.unique():\n",
    "    print('For the %s we have '\n",
    "          'beamhardening correction values of %s' % (scanner,\n",
    "                                                     Data[Data.Scanner==scanner]['BeamHardeningCorrection'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check defect pixel masking parameters\n",
    "for scanner in Data.Scanner.unique():\n",
    "    print('For the %s we have '\n",
    "          'defect pixel masking values of %s' % (scanner,\n",
    "                                                 Data[Data.Scanner==scanner]['DefectPixelMasking'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check defect pixel masking parameters\n",
    "for scanner in Data.Scanner.unique():\n",
    "    print('For the %s we have '\n",
    "          'reconstruction gray values of %s' % (scanner,\n",
    "                                                  Data[Data.Scanner==scanner]['Grayvalue'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check and display scan times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data['Scan time total'] = [ st * stk  for st, stk in zip(Data['Scan time'], Data['Stacks'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # https://www.geeksforgeeks.org/iterating-over-rows-and-columns-in-pandas-dataframe/\n",
    "# columns = list(Data)\n",
    "# columns.remove('Folder') \n",
    "# columns.remove('Sample')\n",
    "# columns.remove('LogFile')\n",
    "# columns.remove('Reconstructions')\n",
    "# columns.remove('Number of reconstructions')\n",
    "# columns.remove('Grayvalue')\n",
    "# columns.remove('Scan time')\n",
    "# columns.remove('Scan time total')\n",
    "# columns.remove('Scan date')\n",
    "# print(columns)\n",
    "# for col in columns:\n",
    "#     print(col)\n",
    "#     print(Data[col].unique())\n",
    "#     print(80*'-')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check voxel sizes (*rounded* to two after-comma values)\n",
    "# If different, spit out which values\n",
    "roundto = 2\n",
    "if len(Data['Voxelsize'].round(roundto).unique()) > 1:\n",
    "    print('We scanned all datasets with %s different voxel sizes' % len(Data['Voxelsize'].round(roundto).unique()))\n",
    "    for vs in sorted(Data['Voxelsize'].round(roundto).unique()):\n",
    "        print('-', vs, 'um for ', end='')\n",
    "        for c, row in Data.iterrows():\n",
    "            if float(vs) == round(row['Voxelsize'], roundto):\n",
    "                print(os.path.join(row['Sample'], row['Scan']), end=', ')\n",
    "        print('')\n",
    "else:\n",
    "    print('We scanned all datasets with equal voxel size, namely %s um.' % float(Data['Voxelsize'].round(roundto).unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if len(Data['Grayvalue'].unique()) > 1:\n",
    "#     print('We reconstructed the datasets with different maximum gray values, namely')\n",
    "#     for gv in Data['Grayvalue'].unique():\n",
    "#         print(gv, 'for Samples ', end='')\n",
    "#         for c, row in Data.iterrows():\n",
    "#             if float(gv) == row['Grayvalue']:\n",
    "#                 print(os.path.join(row['Sample'], row['Scan']), end=', ')\n",
    "#         print('')\n",
    "# else:\n",
    "#     print('We reconstructed all datasets with equal maximum gray value, namely %s.' % Data['Grayvalue'].unique()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data[['Sample', 'Scan',\n",
    "#       'Voxelsize', 'Scanner',\n",
    "#       'Scan date', 'CameraWindow', 'RotationStep', 'Averaging',\n",
    "#       'Scan time', 'Stacks', 'Scan time total']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get an overview over the total scan time\n",
    "# Nice output based on https://stackoverflow.com/a/8907407/323100\n",
    "total_seconds = int(Data['Scan time total'].sum())\n",
    "hours, remainder = divmod(total_seconds,60*60)\n",
    "minutes, seconds = divmod(remainder,60)\n",
    "print('In total, we scanned for %s hours and %s minutes)' % (hours, minutes))\n",
    "for machine in Data['Scanner'].unique():\n",
    "    total_seconds = int(Data[Data['Scanner'] == machine]['Scan time total'].sum())\n",
    "    hours, remainder = divmod(total_seconds,60*60)\n",
    "    minutes, seconds = divmod(remainder,60)\n",
    "    print('\\t - Of these, we scanned %s hours and %s minutes on the %s,'\n",
    "          ' for %s scans' % (hours,\n",
    "                             minutes,\n",
    "                             machine,\n",
    "                             len(Data[Data['Scanner'] == machine])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data[['Sample', 'Scan',\n",
    "      'Voxelsize', 'Scanner',\n",
    "      'Scan date', 'CameraWindow', 'RotationStep', 'Averaging', 'Scan time', 'Stacks' ]].to_excel('Foetus.Details.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data[['Sample', 'Scan',\n",
    "      'Voxelsize', 'Scanner',\n",
    "      'Scan date', 'CameraWindow',\n",
    "      'RotationStep', 'Averaging', 'Scan time', 'Stacks' ]].to_excel(os.path.join(Root,'Foetus.Details.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sort our dataframe by scan date\n",
    "# Data.sort_values(by='Scan date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data['PreviewImagePath'] = [sorted(glob.glob(os.path.join(f, '*_spr.bmp')))[0] for f in Data['Folder']]\n",
    "Data['PreviewImage'] = [dask_image.imread.imread(pip).squeeze()\n",
    "                        if pip\n",
    "                        else numpy.random.random((100, 100)) for pip in Data['PreviewImagePath']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make an approximately square overview image\n",
    "# lines = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for c, row in Data.iterrows():\n",
    "#    plt.subplot(lines, int(numpy.ceil(len(Data) / float(lines))), c + 1)\n",
    "#    plt.imshow(row.PreviewImage.squeeze())\n",
    "#    plt.title(os.path.join(row['Sample'], row['Scan']))\n",
    "#    plt.gca().add_artist(ScaleBar(row['Voxelsize'],\n",
    "#                                  'um',\n",
    "#                                  color='black',\n",
    "#                                  frameon=True))\n",
    "#    plt.axis('off')\n",
    "#plt.tight_layout()\n",
    "#plt.savefig(os.path.join(Root, 'ScanOverviews.Foetus.png'),\n",
    "#            bbox_inches='tight')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data[['Scan date', 'Sample', 'Scan', 'Grayvalue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# skimage.exposure.is_low_contrast(row.Mid_Coronal,\n",
    "#                                  fraction_threshold=0.005,\n",
    "#                                  lower_percentile=1,\n",
    "#                                  upper_percentile=99,\n",
    "#                                  method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load all reconstructions into ephemereal DASK arrays\n",
    "Reconstructions = [None] * len(Data)\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   desc='Load reconstructions',\n",
    "                   total=len(Data)):\n",
    "    Reconstructions[c] = dask_image.imread.imread(os.path.join(row['Folder'],\n",
    "                                                               '*rec*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if something went wrong\n",
    "# for file in Data['OutputNameRec']:\n",
    "#     print(file)\n",
    "#     dask.array.from_zarr(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# How big are the datasets?\n",
    "Data['Size'] = [rec.shape for rec in Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Sample', 'Scan', 'Size', 'ROI']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check defect pixel masking parameters\n",
    "for scanner in Data.Scanner.unique():\n",
    "    print('For the %s we have '\n",
    "          'reconstruction gray values of %s' % (scanner,\n",
    "                                                Data[Data.Scanner==scanner]['Size'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The three cardinal directions\n",
    "directions = ['Axial',\n",
    "              'Coronal',\n",
    "              'Sagittal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read or calculate the middle slices, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Mid_' + direction] = [None] * len(Reconstructions)\n",
    "for c, row in tqdm(Data.iterrows(), desc='Middle images', total=len(Data), leave=False):\n",
    "    for d, direction in tqdm(enumerate(directions),\n",
    "                             desc='%s/%s' % (row['Sample'], row['Scan']),\n",
    "                             leave=False,\n",
    "                             total=len(directions)):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                                   '%s.%s.Middle.%s.png' % (row['Sample'],\n",
    "                                                            row['Scan'],\n",
    "                                                            direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c, 'Mid_' + direction] = dask_image.imread.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate requested axial view\n",
    "            if 'Axial' in direction:\n",
    "                Data.at[c, 'Mid_' + direction] = Reconstructions[c][Data['Size'][c][0] // 2].squeeze().compute()\n",
    "            if 'Coronal' in direction:\n",
    "                Data.at[c, 'Mid_' + direction] = Reconstructions[c][:, Data['Size'][c][1] // 2, :].squeeze().compute()\n",
    "            if 'Sagittal' in direction:\n",
    "                Data.at[c, 'Mid_' + direction] = Reconstructions[c][:, :, Data['Size'][c][2] // 2].squeeze().compute()\n",
    "            # Save the calculated 'direction' view to disk\n",
    "            imageio.imwrite(outfilepath, (Data.at[c, 'Mid_' + direction]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show middle slices\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   desc='Saving middle images overview',\n",
    "                   total=len(Data),\n",
    "                   leave=False):\n",
    "    outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                               '%s.%s.MiddleSlices.png' % (row['Sample'], row['Scan']))\n",
    "    if not os.path.exists(outfilepath):    \n",
    "        for d, direction in tqdm(enumerate(directions),\n",
    "                                 desc='%s/%s' % (row['Sample'], row['Scan']),\n",
    "                                 leave=False,\n",
    "                                 total=len(directions)):\n",
    "            plt.subplot(1, 3, d + 1)\n",
    "            plt.imshow(row['Mid_' + direction].squeeze())\n",
    "            if d == 0:\n",
    "                plt.axhline(row.Size[1] // 2, c=seaborn.color_palette()[0])\n",
    "                plt.axvline(row.Size[2] // 2, c=seaborn.color_palette()[1])\n",
    "                plt.gca().add_artist(ScaleBar(row['Voxelsize'],\n",
    "                                              'um',\n",
    "                                              color=seaborn.color_palette()[2]))\n",
    "            elif d == 1:\n",
    "                plt.axhline(row.Size[0] // 2, c=seaborn.color_palette()[2])\n",
    "                plt.axvline(row.Size[d] // 2, c=seaborn.color_palette()[1])\n",
    "                plt.gca().add_artist(ScaleBar(row['Voxelsize'],\n",
    "                                              'um',\n",
    "                                              color=seaborn.color_palette()[0]))\n",
    "            else:\n",
    "                plt.axhline(row.Size[0] // 2, c=seaborn.color_palette()[2])\n",
    "                plt.axvline(row.Size[d] // 2, c=seaborn.color_palette()[0])\n",
    "                plt.gca().add_artist(ScaleBar(row['Voxelsize'],\n",
    "                                              'um',\n",
    "                                              color=seaborn.color_palette()[1]))\n",
    "            plt.title('%s, %s' % (os.path.join(row['Sample'], row['Scan']),\n",
    "                                  direction + ' Middle slice'))\n",
    "            plt.axis('off')\n",
    "            plt.savefig(outfilepath,\n",
    "                        transparent=True,\n",
    "                        bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read or calculate the directional MIPs, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['MIP_' + direction] = [None] * len(Reconstructions)\n",
    "for c, row in tqdm(Data.iterrows(), desc='MIPs', total=len(Data), leave=False):\n",
    "    for d, direction in tqdm(enumerate(directions),\n",
    "                             desc='%s/%s' % (row['Sample'], row['Scan']),\n",
    "                             leave=False,\n",
    "                             total=len(directions)):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                                   '%s.%s.MIP.%s.png' % (row['Sample'],\n",
    "                                                      row['Scan'],\n",
    "                                                      direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c, 'MIP_' + direction] = dask_image.imread.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate MIP\n",
    "            Data.at[c, 'MIP_' + direction] = Reconstructions[c].max(axis=d).compute()\n",
    "            # Save it out\n",
    "            imageio.imwrite(outfilepath, Data.at[c, 'MIP_' + direction].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show MIP slices\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   desc='Saving MIP images overview',\n",
    "                   total=len(Data),\n",
    "                   leave=False):\n",
    "    outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                               '%s.%s.MIPs.png' % (row['Sample'], row['Scan']))\n",
    "    if not os.path.exists(outfilepath):    \n",
    "        for d, direction in tqdm(enumerate(directions),\n",
    "                                          desc='%s/%s' % (row['Sample'], row['Scan']),\n",
    "                                          leave=False,\n",
    "                                          total=len(directions)):\n",
    "            plt.subplot(1, 3, d + 1)\n",
    "            plt.imshow(row['MIP_' + direction].squeeze())\n",
    "            plt.gca().add_artist(ScaleBar(row['Voxelsize'],\n",
    "                                          'um'))\n",
    "            plt.title('%s, %s' % (os.path.join(row['Sample'], row['Scan']),\n",
    "                                  direction + ' MIP'))\n",
    "            plt.axis('off')\n",
    "        plt.savefig(outfilepath,\n",
    "                    transparent=True,\n",
    "                    bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data.Voxelsize.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop the high-resolution scans for showing the concatenated slices below\n",
    "print('We have %s items in our dataframe' % len(Data))\n",
    "dropsize = 12\n",
    "Data= Data[Data['Voxelsize'] > dropsize]\n",
    "Data.reset_index(drop=True, inplace=True)\n",
    "print('After dropping all datasets with voxelsizes > %s um we now have %s items in our dataframe' % (dropsize, len(Data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reload the reconstructions, so we show the correct slice below\n",
    "Reconstructions = [None] * len(Data)\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   desc='Load reconstructions',\n",
    "                   total=len(Data)):\n",
    "    Reconstructions[c] = dask_image.imread.imread(os.path.join(row['Folder'],\n",
    "                                                               '*rec*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean brightness of the reconstructions\n",
    "# Subsample for speed reasons\n",
    "subsample = 1\n",
    "Data['MeanBrightness'] = [rec[::subsample,::subsample,::subsample].mean().compute() for rec in Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mean brightness of the scan with their dates\n",
    "seaborn.scatterplot(data=Data,\n",
    "                    x='Scan date',\n",
    "                    y='MeanBrightness',\n",
    "                    # size='MeanBrightness'\n",
    "                   )\n",
    "# Label text: https://matplotlib.org/stable/tutorials/text/annotations.html\n",
    "for c,row in Data.iterrows():\n",
    "    plt.gca().annotate(row.Sample.replace('Foetus01', 'F1').replace('_Lugol','').replace('_05pct','').replace('_10pct','').replace('_15pct',''),\n",
    "                       xy=(row['Scan date'], row.MeanBrightness),\n",
    "                       xycoords='data',\n",
    "                       xytext=(-3, -75),\n",
    "                       textcoords='offset points',\n",
    "                       ha='left',\n",
    "                       rotation=-60)\n",
    "plt.ylim(ymin=0)\n",
    "plt.title('Average gray value of the %s-times subsampled reconstructions' % subsample)\n",
    "for o in [OutPutDir, Root]:\n",
    "    plt.savefig(os.path.join(o,\n",
    "                             'MeanBrightness.%s.png' % Data.SampleName.unique()[0]),\n",
    "                transparent=False,\n",
    "                bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for direction in directions:\n",
    "    # Show each directional middle slice, concatenated\n",
    "    # Concatenate figures with GridSpec\n",
    "    # https://matplotlib.org/stable/gallery/subplots_axes_and_figures/gridspec_multicolumn.html#sphx-glr-gallery-subplots-axes-and-figures-gridspec-multicolumn-py\n",
    "    # Set up figure\n",
    "    fig = plt.figure(figsize=(len(Data)*3, 3),\n",
    "                     constrained_layout=False)\n",
    "    gs = GridSpec(1, len(Data), figure=fig, wspace=0, hspace=0)\n",
    "    # plot the selected images\n",
    "    for c, row in Data.iterrows():\n",
    "        fig.add_subplot(gs[0, c])\n",
    "        plt.imshow(row['Mid_%s' % direction].squeeze())\n",
    "        plt.gca().add_artist(ScaleBar(row['Voxelsize'],'um'))\n",
    "        plt.title('%s/%sum\\nMiddle %s slice' % (row.Sample.replace('Foetus02', 'F2').replace('_Lugol', '').replace('pct', '%Lgl'), row.Voxelsize,\n",
    "                                                direction))\n",
    "        plt.axis('off')\n",
    "    plt.savefig(os.path.join(Root, 'Foetus02.Mid_%s.png' % direction),\n",
    "                transparent=False,\n",
    "                bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for direction in directions:\n",
    "    # Show each directional MIP, concatenated\n",
    "    fig = plt.figure(figsize=(len(Data)*3,3), constrained_layout=False)\n",
    "    gs = GridSpec(1,len(Data), figure=fig, wspace=0, hspace=0)\n",
    "    # plot the selected images\n",
    "    for c,row in Data.iterrows():\n",
    "        fig.add_subplot(gs[0, c])\n",
    "        plt.imshow(row['MIP_%s' % direction].squeeze())\n",
    "        plt.gca().add_artist(ScaleBar(row['Voxelsize'],'um'))\n",
    "        plt.title('%s\\n%s MIP' % (row.Sample.replace('Foetus02', 'F2').replace('_Lugol', '_Lgl').replace('pct', '%'),\n",
    "                                  direction))\n",
    "        plt.axis('off')\n",
    "    plt.savefig(os.path.join(Root, 'Foetus02.MIP_%s.png' % direction),\n",
    "                transparent=False,\n",
    "                bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show one slice of each of the scans, concatenated\n",
    "for whichslice in range(250,Data['Number of reconstructions'][3]-250,250):\n",
    "    # Concatenate figures with GridSpec\n",
    "    # https://matplotlib.org/stable/gallery/subplots_axes_and_figures/gridspec_multicolumn.html#sphx-glr-gallery-subplots-axes-and-figures-gridspec-multicolumn-py\n",
    "    from matplotlib.gridspec import GridSpec\n",
    "    # Set up figure\n",
    "    fig = plt.figure(figsize=(len(Data)*3,3), constrained_layout=False)\n",
    "    gs = GridSpec(1,len(Data), figure=fig, wspace=0, hspace=0)\n",
    "    # plot the selected images\n",
    "    for c, row in Data.iterrows():\n",
    "        fig.add_subplot(gs[0, c])\n",
    "        plt.imshow(Reconstructions[c][whichslice])\n",
    "        plt.gca().add_artist(ScaleBar(row['Voxelsize'],'um'))\n",
    "        plt.title('%s\\nReconstruction %s' % (row.Sample.replace('Foetus02', 'F2').replace('_Lugol', '_Lgl').replace('pct', '%'),\n",
    "                                             whichslice))\n",
    "        plt.axis('off')\n",
    "    plt.savefig(os.path.join(Root, 'Foetus02.Slice%04d.png' % whichslice),\n",
    "                transparent=False,\n",
    "                bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out GIF of progression\n",
    "for direction in directions:\n",
    "    # Concatenate directional MIPs into a GIF\n",
    "    # Since we `dask`ed the images, we first need to compute them...\n",
    "    mipseries = [mip.squeeze() for mip in Data['MIP_%s' % direction]]\n",
    "    imageio.mimwrite('Foetus02.Animation.MIP.%s.gif' % direction, mipseries, format= '.gif', fps = 1)\n",
    "    # Concatenate directional middle images into a GIF\n",
    "    # Since we `dask`ed the images, we first need to compute them...\n",
    "    midseries = [mid.squeeze() for mid in Data['Mid_%s' % direction]]\n",
    "    imageio.mimwrite('Foetus02.Animation.Middle.%s.gif' % direction, midseries, format= '.gif', fps = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the histograms of one of the MIPs\n",
    "# Caveat: dask.da.histogram returns histogram AND bins, making each histogram a 'nested' list of [h, b]\n",
    "Data['Histogram'] = [dask.array.histogram(dask.array.array(mip.squeeze()),\n",
    "                                          bins=2**8,\n",
    "                                          range=[0, 2**8]) for mip in Data['MIP_Coronal']]\n",
    "# Actually compute the data and put only h into the dataframe, since we use it quite often below.\n",
    "# Discard the bins\n",
    "Data['Histogram'] = [h.compute() for h,b in Data['Histogram']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c, row in sorted(Data.iterrows()):\n",
    "    plt.semilogy(row.Histogram, label='%s/%s' % (row.Sample, row.Scan))\n",
    "plt.xlim([0, 255])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def overeexposecheck(item, threshold=222, howmanypercent=0.05, whichone='Coronal', verbose=False):\n",
    "    '''Function to check if a certain amount of voxels are brighter than a certain value'''\n",
    "    if (Data['MIP_%s' % whichone][item]>threshold).sum() > (Data['MIP_%s' % whichone][item].size * howmanypercent / 100):\n",
    "        if verbose:\n",
    "            plt.imshow(Data['MIP_%s' % whichone][item].squeeze())\n",
    "            plt.imshow(numpy.ma.masked_equal(Data['MIP_%s' % whichone][item].squeeze()>threshold, False),\n",
    "                       cmap='viridis_r',\n",
    "                       alpha=.618)\n",
    "            plt.title('%s/%s\\n%s px of %s Mpixels (>%s%%) are brighter '\n",
    "                      'than %s' % (Data['Sample'][item],\n",
    "                                   Data['Scan'][item],\n",
    "                                   (Data['MIP_%s' % whichone][item]>threshold).sum().compute(),\n",
    "                                   round(1e-6 * Data['MIP_%s' % whichone][item].size,2),\n",
    "                                   howmanypercent,\n",
    "                                   threshold))\n",
    "            plt.axis('off')\n",
    "            plt.gca().add_artist(ScaleBar(Data['Voxelsize'][item],\n",
    "                                          'um'))\n",
    "            plt.show()\n",
    "        return(True)\n",
    "    else:\n",
    "        return(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if 'too much' of the MIP is overexposed\n",
    "Data['OverExposed'] = [overeexposecheck(c,\n",
    "                                        whichone='Coronal',\n",
    "                                        verbose=True) for c, row in Data.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('At the moment, we have previewed %s *Foetus* scans of %s samples in %s' % (len(Data),\n",
    "                                                                                  len(Data.Sample.unique()),\n",
    "                                                                                  Root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
